"""
Methods to handle linear constraints.

In particular, implements a simple version of the double description method [1] to get the generators
of the nearby tangent cone

[1] K. Fukuda and A. Prodon. Double description method revisited.
    In: Combinatorics and Computer Science (M. Deza, R. Euler, I. Manoussakis eds), Springer 1996
"""
import numpy as np
from scipy.linalg import null_space, qr

from .ds import DEFAULT_PARAMS, EXIT_MAXFUN_REACHED, EXIT_ALPHA_MIN_REACHED


def nearby_constraints(A, b, x, alpha):
    """
    For constraint set {y : A @ y <= b}, determine the nearly active constraints J(x,alpha)

    Returns a list, subset of [1, ..., m] of nearly active constraints (m = total number of constraints)
    """
    m, n = A.shape  # number of constraints, dimension
    assert b.shape == (m,), "A and b have incompatible dimensions"
    assert x.shape == (n,), "A and x have incompatible dimensions"
    assert alpha > 0.0, "alpha must be strictly positive"
    assert np.all(A @ x <= b), "x must be feasible, Ax<=b"
    s = b - A @ x  # all >= 0
    J = []
    for j in range(m):
        aj = A[j, :]
        if s[j] <= alpha * np.dot(aj, aj):
            J.append(j)
    return J


def full_rank_generators(A):
    """
    Matrix A is n*m with m <= n, and A has full (column) rank

    Given the cone generated by columns of A, return a set of generators for the corresponding polar cone

    The generators are the columns of the returned matrix
    """
    n, m = A.shape
    assert m <= n, "A must be size n*m with m <= n"
    if m == 0:
        # cone is {0}, so polar cone is all of R^n --> take any PSS for R^n as a valid set of generators
        return np.hstack((np.eye(n), -np.eye(n)))
    assert np.linalg.matrix_rank(A) == m, "A must be full rank"
    Apinv = np.linalg.pinv(A.T)
    # print("-Apinv =")
    # print(-Apinv)
    if m < n:
        null = null_space(A.T)
        # print("null =")
        # print(null)
        return np.hstack((-Apinv, null, -null))
    else:
        return -Apinv


def calculate_cone_generators(A, verbose=False):
    """
    Compute generators for a pointed cone. Given
        R = calculate_cone_generators(A)
    the columns of R are a minimal generating set for the cone {y : A @ y >= 0}.

    Uses a simple implementation of the double description method [2], or the simple approach from [1] if we do not
    have many constraints.

    References:
    [1] C. P. Dobler. A Matrix Approach to Finding a Set of Generators and Finding the Polar (Dual) of a Class of
        Polyhedral Cones. SIAM J. Matrix Anal. & Appl. 15:3 (1994), pp. 796-803.
    [2] K. Fukuda and A. Prodon. Double description method revisited.
        In: Combinatorics and Computer Science (M. Deza, R. Euler, I. Manoussakis eds), Springer 1996

    Inputs:
    - A: m*n matrix of linear inequalities, where m is the number of constraints and n is the dimension of the space

    Outputs:
    - R: Generating set of the cone, matrix of size n*p where p is the number of generators
    """
    m, n = A.shape
    if m == 0:
        # No constraints, cone = R^n
        if verbose:
            print("No constraints, cone is R^%g" % n)
        I_n = np.eye(n)
        return np.hstack((I_n, -I_n))
    elif m < n:
        # Not full rank set of constraints, use Dobler 1994
        # Generators are columns of pinv(A) and +/- any basis for nul(A)
        if verbose:
            print("Incomplete set of constraints, using simple generator formula")
        Apinv = np.linalg.pinv(A)
        # print("A.T =", A.T)
        null = null_space(A)
        # print("null =")
        # print(null)
        return np.hstack((Apinv, null, -null))

    assert np.linalg.matrix_rank(A) == n, "A must have full column rank for this to work"

    if m == n:
        # A invertible, use Dobler 1994 approach without needing null space
        if verbose:
            print("Simple generator formula")
        return np.linalg.pinv(A)

    # m > n case: first find a set of n linearly independent constraints, then add in the remainder
    _, _, piv = qr(A.T, mode='economic', pivoting=True)
    J = piv[:n]
    R = np.linalg.pinv(A[J, :])
    if verbose:
        print("Initially selecting n linearly independent constraints", J)
        print("R =")
        print(R)
    cons_to_add = np.setdiff1d(list(range(m)), J)

    # Add each remaining constraint (not in J), one at a time
    # At the start of each iteration, the columns of R are a generating set for the cone given by inequalities A[J, :]

    ZERO_THRESH = 10.0 * np.finfo(float).eps  # for determining Iplus, Izero and Iminus
    for j in cons_to_add:
        if verbose:
            print("Adding constraint j = %g" % j, ", aj =", A[j, :])
        v = A[j, :] @ R
        if verbose:
            print("v =", v)
        ka = np.linalg.matrix_rank(A[J, :])

        Iplus = np.nonzero(v > ZERO_THRESH)[0]
        Izero = np.nonzero(np.abs(v) <= ZERO_THRESH)[0]
        # Iminus = np.nonzero(np.abs(v) < ZERO_THRESH)[0]
        Iminus = np.setdiff1d(list(range(R.shape[1])), np.hstack((Iplus, Izero)))  # remaining indices
        if verbose:
            print("Iplus =", Iplus, "Izero =", Izero, "Iminus =", Iminus)

        # Finding the adjacent vectors
        Rnew = np.zeros((n, 0), dtype=float)
        Jnew = np.append(J, j)  # new index set (np.append does not modify J)

        for ip in range(len(Iplus)):
            r1 = R[:, Iplus[ip]]
            ar1 = A[Jnew, :] @ r1
            z1 = np.nonzero(np.abs(ar1[:-1]) <= ZERO_THRESH)[0]
            k1 = np.linalg.matrix_rank(A[J[z1], :]) if len(z1) > 0 else 0

            for im in range(len(Iminus)):
                r2 = R[:, Iminus[im]]
                ar2 = A[Jnew, :] @ r2
                z2 = np.nonzero(np.abs(ar2[:-1]) <= ZERO_THRESH)[0]
                k2 = np.linalg.matrix_rank(A[J[z2], :]) if len(z2) > 0 else 0

                iz = np.intersect1d(z1, z2)
                kz = np.linalg.matrix_rank(A[J[iz], :]) if len(iz) > 0 else 0

                if k1 == ka - 1 and k2 == ka - 1 and kz == ka - 2:
                    # r1 and r2 are adjacent directions
                    r3 = ar1[-1] * r2 - ar2[-1] * r1
                    if np.linalg.norm(r3) > ZERO_THRESH:
                        Rnew = np.hstack((Rnew, r3.reshape((n, 1))))

        # Building the new set
        R = np.hstack((R[:, Iplus], R[:, Izero], Rnew))
        J = Jnew.copy()

        if verbose:
            print("After adding j=%g" % j)
            print("A = ")
            print(A[J, :])
            print("R =")
            print(R)

    return R


def get_poll_directions(A, b, x, alpha, include_negative_directions=True, verbose=False):
    """
    Given feasible region { y : A @ y <= b }, a feasible point x and radius alpha, return a useful set of
    feasible poll directions in B(x,alpha).

    D, Dneg = get_poll_directions(A, b, x, alpha)

    :param A: m*n matrix defining the inequality constraints
    :param b: length-m vector defining the inequality constraints
    :param x: length-n vector for the current iterate
    :param alpha: radius of search region, alpha > 0
    :return: D, n*p matrix (some p) of vectors in B(0,alpha) such that all poll points x+D[:,i] are feasible
    :return: Dneg, n*p2 matrix (some p2) similar to D, but corresponding to negative of tangent directions. Possibly None
    """
    ZERO_THRESH = 5.0 * np.finfo(float).eps  # for measuring distance to boundary
    m, n = A.shape
    assert b.shape == (m,), "b has incompatible shape with A"
    assert x.shape == (n,), "x has incompatible shape with A"
    assert alpha > 0.0, "alpha must be strictly positive"
    assert np.all(A @ x <= b + ZERO_THRESH), "x must be feasible"

    J = nearby_constraints(A, b, x, alpha)  # nearly active constraints at x
    if verbose:
        print("Nearly active constraints are", J)
    N = A[J, :]  # generators of the normal cone <--> the tangent cone is given by N @ x <= 0
    T = calculate_cone_generators(-N)  # columns of T are generators of the tangent cone

    # Scale each column of T to length alpha
    T = T * alpha / np.linalg.norm(T, axis=0)
    if verbose:
        print("T =")
        print(T)
        print("T has %g generators" % T.shape[1])

    # Calculate scaled length of -T directions to ensure feasibility
    s = np.maximum(b - A @ x, 0.0)  # slack variables at x, ensure floored at zero in case of rounding errors
    if T.shape[1] > 0:
        if include_negative_directions:
            Tneg = np.zeros((n, 0), dtype=float)
            for i in range(T.shape[1]):
                ti = T[:, i]
                if verbose:
                    print("Negative of direction i=%g" % i, "ti =", ti)
                if np.any(T.T @ ti <= -(1.0 - ZERO_THRESH) * alpha**2):
                    # Found another column tj such that dot(tj, ti) == -alpha^2, i.e. tj = -ti (since both have length alpha)
                    # No need to add -ti to poll directions, since it's already there
                    if verbose:
                        print("Found -ti already in T, skipping")
                    continue
                A_ti = A @ ti
                idx = np.nonzero(A_ti < -ZERO_THRESH)[0]
                alpha_i = np.min(s[idx] / (-A_ti[idx])) if len(idx) > 0 else 1.0
                alpha_i = max(min(alpha_i, 1.0), 0.0)  # always ensure 0 <= alpha_i <= 1
                if verbose:
                    # print("x =", x, ", s =", s)
                    # print("A_ti =", A_ti)
                    # print("idx =", idx)
                    # print("s / (-A_ti) =", s[idx] / (-A_ti[idx]))
                    print("alpha_i = %g" % alpha_i)
                    # print("-alpha_i ti =", -alpha_i * ti)
                    # print("new point =", x - alpha_i * ti)
                if alpha_i > ZERO_THRESH:
                    Tneg = np.hstack((Tneg, -alpha_i * ti.reshape((n, 1))))

            return T, Tneg
        else:
            # Exclude negative directions, for comparison purposes
            return T, None
    else:
        # Tangent cone is {0}, i.e. normal cone spans R^n
        # i.e. use these as the poll directions (after suitable scaling)
        if verbose:
            print("Trivial tangent cone, using outward normals as poll directions")
        normal_dirns = N.T  # columns are outward normal directions
        normal_dirns = normal_dirns * alpha / np.linalg.norm(normal_dirns, axis=0)  # scale to length alpha
        poll_dirns = np.zeros((n, 0), dtype=float)
        for i in range(normal_dirns.shape[1]):
            ni = normal_dirns[:, i]
            if verbose:
                print("Normal i=%g" % i, "ni =", ni)
            A_ni = A @ ni
            idx = np.nonzero(A_ni > ZERO_THRESH)[0]
            alpha_i = np.min(s[idx] / (A_ni[idx])) if len(idx) > 0 else 1.0
            alpha_i = max(min(alpha_i, 1.0), 0.0)  # always ensure 0 <= alpha_i <= 1
            if verbose:
                print("alpha_i = %g" % alpha_i)
            if alpha_i > ZERO_THRESH:
                poll_dirns = np.hstack((poll_dirns, alpha_i * ni.reshape((n, 1))))

        return poll_dirns, None


def ds_lincons(f, x0, A=None, b=None,
               rho=DEFAULT_PARAMS['rho'],
               maxevals=DEFAULT_PARAMS['maxevals'],
               alpha0=DEFAULT_PARAMS['alpha0'],
               alpha_max=DEFAULT_PARAMS['alpha_max'],
               alpha_min=DEFAULT_PARAMS['alpha_min'],
               gamma_inc=DEFAULT_PARAMS['gamma_inc'],
               gamma_dec=DEFAULT_PARAMS['gamma_dec'],
               verbose=DEFAULT_PARAMS['verbose'],
               print_freq=DEFAULT_PARAMS['print_freq'],
               rho_uses_normd=DEFAULT_PARAMS['rho_uses_normd']):
    """
        A generic direct-search code for linearly constrained black-box optimization.

            x, fx, nf, flag = ds_lincons(f, x0, A, b)

        attempts to minimize the function f starting at x0, subject to the linear inequality
        constraints A @ x <= b, using a direct-search approach. The method is based on moves along certain
        polling directions: these moves are controlled by means of an adaptive stepsize.

        Inputs:
            f: Function handle for the objective to be minimized.
            x0: Initial point. Must satisfy constraints A @ x0 <= b
            A: matrix defining linear inequality constraints A @ x <= b. Default: None
            b: right-hand side defining linear inequality constraints A @ x <= b. Default: None
            rho: Choice of the forcing function.
                Default: see DEFAULT_PARAMS['rho']
            maxevals: Maximum number of calls to f performed by the algorithm.
                Default: see DEFAULT_PARAMS['maxevals']
            alpha0: Initial value for the stepsize parameter.
                Default: See DEFAULT_PARAMS['alpha0']
            alpha_max: Maximum value for the stepsize parameter.
                Default: See DEFAULT_PARAMS['alpha_max']
            alpha_min: Minimum value for the stepsize parameter.
                Default: See DEFAULT_PARAMS['alpha_min']
            gamma_inc: Increase factor for the stepsize update.
                Default: See DEFAULT_PARAMS['gamma_inc']
            gamma_dec: Decrease factor for the stepsize update.
                Default: See DEFAULT_PARAMS['gamma_dec']
            verbose: Boolean indicating whether information should be displayed
            during an algorithmic run.
                Default: See DEFAULT_PARAMS['verbose']
            print_freq: Value indicating how frequently information should
            be displayed.
                Default: See DEFAULT_PARAMS['print_freq']
            rho_uses_normd: Boolean indicating whether the forcing function should
            account for the norm of the direction.
                Default: See DEFAULT_PARAMS['rho_uses_normd']

        Outputs:
            x: Best solution found (vector of same dimension than x0).
            fx: Value of f at x.
            nf: Number of function evaluations that have been used.
            stopping flag: Indicator of the reason why the method stopped.
                EXIT_MAXFUN_REACHED: The maximum number of function evaluations
                was reached.
                EXIT_ALPHA_MIN_REACHED: The stepsize reached the minimum
                allowed value.

    """

    ###############
    # Initialization
    # Set some sensible defaults for: sufficient decrease threshold, # evaluations, initial step size
    # Set the forcing function
    rho_uses_normd = bool(rho_uses_normd)
    if rho is None:
        if rho_uses_normd:
            rho_to_use = lambda t, normd: min(1e-5, 1e-5 * (t * normd) ** 2)
        else:
            rho_to_use = lambda t: 1e-5 * t ** 2
    else:
        rho_to_use = rho

    # Force correct types
    x = np.array(x0, dtype=float)
    n = len(x)
    x = x.reshape((n,))
    if A is not None:
        A = np.array(A, dtype=float)
        assert b is not None, "b must be provided if A is provided"
    else:
        A = np.zeros((0, n), dtype=float)
        assert b is None, "b must be None if A is None"
    assert A.ndim == 2, "A must be a matrix"
    m = A.shape[0]  # number of linear inequality constraints
    assert A.shape[1] == n, "Second dimension of A must match length of x0"
    if b is not None:
        b = np.array(b, dtype=float)
    else:
        b = np.zeros((0,), dtype=float)
    assert b.ndim == 1, "b must be a vector"
    assert len(b) == m, "Length of b must match first dimension of A"
    alpha_max = float(alpha_max)
    alpha_min = float(alpha_min)
    gamma_inc = float(gamma_inc)
    gamma_dec = float(gamma_dec)
    verbose = bool(verbose)

    # Compute the maximum number of evaluations according to the problem dimension
    if maxevals is None:
        maxevals = min(100 * (n + 1), 1000)
    maxevals = int(maxevals)

    # Set initial stepsize so that it satisfies the bounds
    # alpha_min <= alpha_0 <= alpha_max
    if alpha0 is None:
        alpha0 = 0.1 * max(np.max(np.abs(x0)), 1.0)
        alpha0 = max(min(alpha0, alpha_max), alpha_min)
    alpha0 = float(alpha0)

    # Set frequence of information display
    if print_freq is None:
        print_freq = max(int(maxevals // 20), 1)
    print_freq = int(print_freq)

    # Input checking
    assert callable(f), "Objective function should be callable"
    assert callable(rho_to_use), "Sufficient decrease function rho should be callable"
    assert np.all(A @ x0 <= b + 1e-14), "Initial point must be feasible, A @ x0 <= b"
    assert maxevals > 0, "maxevals should be strictly positive"
    assert alpha_max > 0.0, "alpha_max should be strictly positive"
    assert alpha0 > 0.0, "alpha0 should be strictly positive"
    assert alpha_min > 0.0, "alpha_max should be strictly positive"
    assert alpha_min <= alpha_max, "alpha_min should be <= alpha_max"
    assert alpha0 >= alpha_min, "alpha0 should be >= alpha_min"
    assert gamma_inc >= 1.0, "gamma_inc should be at least 1"
    assert gamma_dec > 0.0, "gamma_dec should be strictly positive"
    assert gamma_dec < 1.0, "gamma_dec should be strictly < 1"
    if verbose:
        assert print_freq > 0, "print_freq should be strictly positive"

    ###################################
    # Start of the optimization process
    fx = f(x)
    nf = 1
    if nf >= maxevals:
        if verbose:
            print("Quit (max evals)")
        return x, fx, nf, EXIT_MAXFUN_REACHED

    ###############
    # Main loop
    alpha = alpha0
    k = -1
    if verbose:
        print("{0:^5}{1:^15}{2:^15}".format('k', 'f(xk)', 'alpha_k'))
    iteration_counts = {'successful': 0, 'successful_negative_direction': 0, 'unsuccessful': 0}
    while nf < maxevals:
        k += 1
        if verbose and k % print_freq == 0:
            print("{0:^5}{1:^15.4e}{2:^15.2e}".format(k, fx, alpha))

        # Generate poll directions adapted to linear constraints
        Dk, Dk_neg = get_poll_directions(A, b, x, alpha, verbose=False)
        # WARNING: poll directions Dk[:,i] are already scaled by alpha, don't multiply by alpha below
        # print("******")
        # print("At x =", x, ", alpha = %g" % alpha)
        # print("Poll directions =")
        # print(Dk)
        # print("******")

        # Start poll step
        ndirs1 = Dk.shape[1]
        ndirs2 = Dk_neg.shape[1] if Dk_neg is not None else 0
        ndirs = ndirs1 + ndirs2

        # Regular direct search - Sufficient decrease, adaptive stepsize
        polling_successful = False
        used_negative_direction = False
        for j in range(ndirs):
            dj = Dk[:, j] if j < ndirs1 else Dk_neg[:, j-ndirs1]
            xnew = x + dj  # WARNING: Dk is already scaled by alpha, so don't multiply by alpha here (different to ds.py)
            fnew = f(xnew)
            nf += 1
            # Compute the target improvement
            sufficient_decrease = (fnew < fx - rho_to_use(alpha, np.linalg.norm(dj))) if rho_uses_normd else (
                        fnew < fx - rho_to_use(alpha))

            # Quit on budget (update to xnew if we just saw an improvement)
            if nf >= maxevals:
                if sufficient_decrease:
                    x = xnew.copy()
                    fx = fnew
                if sufficient_decrease:
                    if used_negative_direction:
                        iteration_counts['successful_negative_direction'] += 1
                    else:
                        iteration_counts['successful'] += 1
                else:
                    iteration_counts['unsuccessful'] += 1
                if verbose:
                    print("{0:^5}{1:^15.4e}{2:^15.2e} - max evals reached".format(k, fx, alpha))
                return x, fx, nf, EXIT_MAXFUN_REACHED

            # If sufficient decrease, update xk and stop poll step
            if sufficient_decrease:
                x = xnew.copy()
                fx = fnew
                # Found a better point=> Possibly increase the stepsize
                alpha = min(gamma_inc * alpha, alpha_max)
                polling_successful = True
                if j >= ndirs1:
                    used_negative_direction = True
                break  # stop poll step, go to next iteration

        # Determine iteration type
        if polling_successful:
            if used_negative_direction:
                iteration_counts['successful_negative_direction'] += 1
            else:
                iteration_counts['successful'] += 1
        else:
            iteration_counts['unsuccessful'] += 1

        # If here, no decrease found
        if alpha < alpha_min:
            if verbose:
                print("{0:^5}{1:^15.4e}{2:^15.2e} - small alpha reached".format(k, fx, alpha))
            break  # finish algorithm
            # Note - Could return here

        if not polling_successful:
            # No better found point=> Decrease the stepsize
            alpha = gamma_dec * alpha
        # End loop
        ###########

    return x, fx, nf, EXIT_ALPHA_MIN_REACHED, iteration_counts


def simplex_example():
    # Simplex example
    m = 3
    n = 2
    A = np.array([[-1.0, 0.0], [0.0, -1.0], [1.0, 1.0]])
    b = np.array([0.0, 0.0, 1])
    # j = 0  <-->  -e1^T x <= 0  (i.e. x1 >= 0)
    # j = 1  <--> -e2^T x <= 0 (i.e. x2 >= 0)
    # j = 2  <--> e^T x <= 1  (i.e. x1+x2 <= 1)
    # x = np.array([0.02, 0.96])
    # x = np.array([0.5, 0.5])
    # x = np.array([0.05, 0.05])
    x = np.array([0.2, 0.2])
    alpha = 0.1
    J = nearby_constraints(A, b, x, alpha)  # expect [0, 2]
    N = A[J, :].T  # columns are generators of normal cone
    print("nearby cons = ", J)
    print("N generators =")
    print(N)
    T = full_rank_generators(N)  # columns are generators of tangent cone
    T = T * alpha / np.linalg.norm(T, axis=0)  # scale so columns all have length alpha
    print("T generators =")
    print(T)
    print("poll points (alpha = %g)" % alpha)
    print("x =", x)
    for i in range(T.shape[1]):
        print("x + di =", x + T[:, i])
    return


def dd_example():
    # Double description examples
    # TODO warning, convention is cone = { y : A @ y >= 0 } (not <= 0)

    print("*** 01 C = {0} ***")
    A = np.array([[1.0, 0.0],  # x1 >= 0
                  [0.0, 1.0],  # x2 >= 0
                  [-1.0, -1.0]])  # x1 + x2 <= 0
    R = calculate_cone_generators(A)
    print("A = (%g constraints)" % A.shape[0])
    print(A)
    print("R (%g generators) =" % R.shape[1])
    print(R)

    print("*** 02 C = R^2 ***")
    A = np.zeros((0, 2))  # no constraints
    R = calculate_cone_generators(A)
    print("A = (%g constraints)" % A.shape[0])
    print(A)
    print("R (%g generators) =" % R.shape[1])
    print(R)

    print("*** 03 Medium case 1 ***")
    A = np.array([[1.0, 0.0]])  # x1 >= 0
    R = calculate_cone_generators(A)
    print("A = (%g constraints)" % A.shape[0])
    print(A)
    print("R (%g generators) =" % R.shape[1])
    print(R)

    print("*** 04 Medium case 1a ***")
    A = np.array([[-1.0, 0.0],  # x1 <= 0
                  [0.0, 1.0],   # x2 >= 0
                  [0.0, -1.0]]) # x2 <= 0
    R = calculate_cone_generators(A)
    print("A = (%g constraints)" % A.shape[0])
    print(A)
    print("R (%g generators) =" % R.shape[1])
    print(R)

    print("*** 05 Medium case 2 ***")
    A = np.array([[-1.0, 0.0],  # x1 <= 0
                  [0.0, -1.0]])  # x2 <= 0
    R = calculate_cone_generators(A)
    print("A = (%g constraints)" % A.shape[0])
    print(A)
    print("R (%g generators) =" % R.shape[1])
    print(R)

    print("*** 06 Medium case 3 ***")
    A = np.array([[-1.0, -1.0]])  # x1 + x2 <= 0
    R = calculate_cone_generators(A)
    print("A = (%g constraints)" % A.shape[0])
    print(A)
    print("R (%g generators) =" % R.shape[1])
    print(R)

    print("*** 07 Medium case 3a ***")
    A = np.array([[0.0, 0.0],  # nothing achieved here
                  [1.0, 1.0],  # x1 + x2 >= 0
                  [-1.0, 1.0],  # x2 - x1 >= 0, i.e. x2 >= x1
                  [1.0, -1.0]])  # x1 - x2 <= 0, i.e. x1 >= x2
    # A = np.array([[1.0, 1.0],  # x1 + x2 >= 0
    #               [-1.0, 1.0],  # x2 - x1 >= 0, i.e. x2 >= x1
    #               [1.0, -1.0]])  # x1 - x2 <= 0, i.e. x1 >= x2
    R = calculate_cone_generators(A)
    print("A = (%g constraints)" % A.shape[0])
    print(A)
    print("R (%g generators) =" % R.shape[1])
    print(R)

    print("*** 08 Hard case 1 ***")
    A = np.array([[1.0, 0.0],  # x1 >= 0
                  [-1.0, -1.0]])  # x1 + x2 <= 0
    R = calculate_cone_generators(A)
    print("A = (%g constraints)" % A.shape[0])
    print(A)
    print("R (%g generators) =" % R.shape[1])
    print(R)

    print("*** 09 Hard case 1a ***")
    A = np.array([[0.0, 1.0],  # x2 >= 0
                  [-1.0, 1.0]])  # x2 - x1 >= 0, i.e. x2 >= x1
    R = calculate_cone_generators(A)
    print("A = (%g constraints)" % A.shape[0])
    print(A)
    print("R (%g generators) =" % R.shape[1])
    print(R)

    print("*** 10 Hard case (3d) 1 ***")
    A = np.array([[1.0, 0.0, 0.0],  # x1 >= 0
                  [0.0, 1.0, 0.0],  # x2 >= 0
                  [-1.0, -1.0, -1.0]])  # x1 + x2 + x3 <= 0
    R = calculate_cone_generators(A)
    print("A = (%g constraints)" % A.shape[0])
    print(A)
    print("R (%g generators) =" % R.shape[1])
    print(R)

    print("*** 11 Pyramid 3d ***")
    L = 3.0
    A = np.array([[-1.0, 0.0, -L],  # x1 + L*x3 <= 0
                  [1.0, 0.0, -L],  # -x1 + L*x3 <= 0
                  [0.0, -1.0, -L],  # x2 + L*x3 <= 0
                  [0.0, 1.0, -L]])  # -x2 + L*x3 <= 0
    R = calculate_cone_generators(A)
    print("A = (%g constraints)" % A.shape[0])
    print(A)
    print("R (%g generators) =" % R.shape[1])
    print(2*L*R)  # normalized to match my slides, expect (+/-L, +/-L, -1)
    return


def poll_set_example():
    np.set_printoptions(precision=5, suppress=True)
    # 2d simplex
    A = np.array([[-1.0, 0.0],
                  [0.0, -1.0],
                  [1.0, 1.0]])
    b = np.array([0.0, 0.0, 1.0])

    print("*** 01 Simple case 1 ***")
    x = np.array([0.3, 0.3])
    alpha = 0.1
    D = get_poll_directions(A, b, x, alpha)
    print("D =")
    print(D)

    print("*** 02 Simple case 2 ***")
    x = np.array([0.3, 0.3])
    alpha = 5.0
    D = get_poll_directions(A, b, x, alpha)
    print("D =")
    print(D)

    print("*** 03 Medium case 1 ***")
    x = np.array([0.01, 0.5])
    alpha = 0.1
    D = get_poll_directions(A, b, x, alpha)
    print("D =")
    print(D)

    print("*** 04 Medium case 2 ***")
    x = np.array([0.01, 0.01])
    alpha = 0.1
    D = get_poll_directions(A, b, x, alpha)
    print("D =")
    print(D)

    print("*** 05 Medium case 3 ***")
    x = np.array([0.49, 0.49])
    alpha = 0.1
    D = get_poll_directions(A, b, x, alpha)
    print("D =")
    print(D)

    print("*** 07 Hard case 1 ***")
    x = np.array([0.01, 0.98])
    alpha = 0.1
    D = get_poll_directions(A, b, x, alpha)
    print("D =")
    print(D)

    print("*** 08 Hard case 2 ***")
    x = np.array([0.01, 0.99])
    alpha = 0.1
    D = get_poll_directions(A, b, x, alpha)
    print("D =")
    print(D)

    # 3d simplex
    A = np.array([[-1.0, 0.0, 0.0],
                  [0.0, -1.0, 0.0],
                  [0.0, 0.0, -1.0],
                  [1.0, 1.0, 1.0]])
    b = np.array([0.0, 0.0, 0.0, 1.0])

    print("*** 09 Hard case 1 (3d) ***")
    x = np.array([0.01, 0.01, 0.95])
    alpha = 0.1
    D = get_poll_directions(A, b, x, alpha)
    print("D =")
    print(D)

    print("*** 10 Hard case 2 (3d) ***")
    x = np.array([0.02, 0.02, 0.96])
    alpha = 0.1
    D = get_poll_directions(A, b, x, alpha)
    print("D =")
    print(D)

    # 3d pyramid
    print("*** 11 Pyramid (3d) ***")
    L = 2.0
    A = np.array([[0.0, 0.0, -1.0],  # z >= 0  <-->  -z <= 0
                  [1.0, 0.0, 0.0],  # x <= L
                  [-1.0, 0.0, 0.0],  # x >= -L  <-->  -x <= L
                  [0.0, 1.0, 0.0],  # y <= L
                  [0.0, -1.0, 0.0], # y >= -L  <-->  -y <= L
                  [1.0, 0.0, L],  # x + Lz <= L
                  [-1.0, 0.0, L], # -x + Lz <= L
                  [0.0, 1.0, L], # y + Lz <= L
                  [0.0, -1.0, L]]) # -y + Lz <= L
    b = np.array([0.0, L, L, L, L, L, L, L, L])

    x = np.array([0.0, 0.0, 0.98])
    alpha = 0.1
    D = get_poll_directions(A, b, x, alpha)
    print("D =")
    print(D)

    # Fig 8.6a from Kolda et al (SIREV, 2003)
    print("*** 12 Kolda et al ***")
    A = np.array([[0.0, 1.0],  # x2 <= 1
                 [1.0, 1.0]])  # x1 + x2 <= 2
    b = np.array([1.0, 2.0])
    x = np.array([0.95, 0.99])
    alpha = 0.1
    D = get_poll_directions(A, b, x, alpha)
    print("D =")
    print(D)
    return


def main():
    # simplex_example()
    # dd_example()
    poll_set_example()
    print("Done")
    return


if __name__ == '__main__':
    main()
